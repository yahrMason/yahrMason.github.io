---
title: Smoothing Splines
permalink: /:collection/:name/
redirect_from: tutorials/15-smoothing-splines/
weave_options:
  error : false
---

## Aknowledgements
This tutorial is heavily inspired by examples from Richard McElreath's [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) text, specifically his spline example in Chapter 4.
Max Lapan has developed and mantains [Julia translations](https://shmuma.github.io/rethinking-2ed-julia/) of the Statistical Rethinking. His notes on chapter 4 were very helpful in the development of this tutorial.
The code has been adapted from the R code in Simon Wood's text, [Generalized Additive Models: An Introduction with R](https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1498728332).
Last but certainly not least, the concepts in this tutorial were inspired by Tristan Mahr's [blog post on smoothing splines](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/). In his post he provides a thorough overview of this type of model.

## Introduction
[Smoothing Splines](https://en.wikipedia.org/wiki/Smoothing_spline) are tools that allow us to model non-linear patterns in data.
A common approach to building splines is to use [basis functions](https://en.wikipedia.org/wiki/Basis_function#:~:text=In%20mathematics%2C%20a%20basis%20function,linear%20combination%20of%20basis%20vectors.), which allow us to build arbitrarily "wiggly" curves to fit the data.
Smoothing splines help avoid over-fitting by regularizing the wiggliness of the curves (typically through penalities on the second derivative at the knots).

This tutorial will demonstrate how Turing and Bayesian Hierarchcial Modeling can be used to fit penalized, smooth splines to model the day of year of first blossom for cherry trees in Japan.
The tutorial assumes a baseline understanding of what B-Splines are and how to fit them to data. The resources in the [Aknowledgements](##Aknowledgements) are all great tools to learn more on the subject.
The data set is used in Chapter 6 of [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). We will load the data via the `StatisticalRethinking` Julia package.

## Imports

```Julia
# Import Turing and Distributions.
using Turing, Distributions

# Import StatisticalRethinking and other packages for datasets.
using StatisticalRethinking, CSV, DataFrames

# Import MCMCChains, Plots, and StatPlots for visualizations and diagnostics.
using MCMCChains, Plots, StatsPlots

# Functionality for constructing arrays with identical elements efficiently.
using FillArrays

# Functionality for working with scaled identity matrices.
using LinearAlgebra

# Functionality for working with splines
using BSplines

# Functionality for fitting linear models
using GLM

# Set a seed for reproducibility.
using Random
Random.seed!(0);
```

## Data

As mentioned previously, we will load the `cherry_blossoms` dataset from the `StatisticalRethinking` package. We will do some basic data cleaning and look at the first few rows.

```Julia
# Import the cheery_blossoms dataset.
data = CSV.read(sr_datadir("cherry_blossoms.csv"), DataFrame);

# drop records that have missing day of year values
data = data[(data.doy .!= "NA"),:];

# convert day of year to numeric column
data[!,:doy] = parse.(Float64,data[!,:doy]);

# Show the first five rows of the dataset.
first(data, 5)
```

Our goal is to model the relationship between `doy` (Day of Year) and `year` variables.
Let's plot the relationship between these columns of the data.

```Julia
# Create x and y variables
x = data.year;
y = data.doy;

# Make a function to make a scatterplot of the raw data
function PlotData(; kwargs...)
    scatter(x,y,label = "Data"; kwargs...)
    xlabel!("Year")
    ylabel!("Day of First Blossom")
end;

# Plot the data
PlotData()
```

From the plot we can see what appears to be a non-linear relationship between the year and day of first blossom.
We will explore this relationship using B-Splines

## Building the Basis 

There are two parameters we need to define our basis with the `BSplines` package. These parameters are the number of knots (or breakpoints) and the order of the basis functions.
It should be noted that the order of a basis function is equal to the degree plus 1. For example, if we want to use cubic basis functions (degree 3), the order is equal to `3+1=4`.

For these data, we will use a cubic spline (order = 4). But how many knots should we choose? In this case, we will use 25 knots. The choice for using a relatively large number of knots will produce a rather "wiggly" fit.  In Chapter 4 section 2 of his text, Simon Wood recommends choosing a number of knots that is larger than needed and induce smoothing later.

The following code builds the knots at the quantiles of the independent variable and builds the basis.
We can simply call the `plot` fuction on the `basis` object to view the basis functions.

```Julia
# Parameters to define basis
N_KNOTS = 15;
ORDER = 4; # cubic basis

# Build a list of the Knots (breakpoints)
KnotsList = quantile(x, range(0, 1; length=N_KNOTS));

# Define the Basis Object
Basis = BSplineBasis(ORDER, KnotsList);

# Assign a constant for the number of basis functions 
# (Number of knots + 2)
n_basis = length(Basis);

# Plot the Basis Functions
plot(Basis, title = "Basis Functions", legend = false)
xlabel!("Year")
```

In order to fit our spline regression model, we need to build a design matrix from our basis object.
The following code builds our design matrix and plots the matrix using the `heatmap` function.

```Julia
# Build a Matrix representation of the Basis
splines = vec(
    mapslices(
        x -> Spline(Basis,x), 
        diagm(ones(n_basis)),
        dims=1
    )
);
X = hcat([s.(x) for s in splines]...);

# Plot the basis matrix using the heatmap function
heatmap(X, title = "Basis Design Matrix", yflip = true)
ylabel!("Observation Number")
xlabel!("Basis Function")
```

Now that we have a design matrix built from our basis functions, we can fit our model.

## Standard Spline

The first model we will fit is a simple regression model using OLS. This model won't have a smoothing penalty applied, and will serve as a comparative baseline for the penalized smoothing splines that we fit later.
We can generate our spline function by multipling the basis design matrix by the regression coefficients. We will construct our Bayesian model using Turing.

The Turing model takes the following arguments:

  - `X` is our spline basis design matrix;
  - `y` is the element we want to predict;
  - `μ_int` is the center of the intercept prior;
  - `σ_prior` is the standard deviation of coeffecient priors;
  - `err_prior` defines the exponential prior on likelihood error.  

```Julia
@model function model_spline(X, y, μ_int, σ_prior, err_prior)

    ## Priors
    # Intercept
    α ~ Normal(μ_int, σ_prior)

    # Spline Coefficients
    β ~ MvNormal(zeros(size(X)[2]), σ_prior)
    
    # Error
    σ ~ Exponential(err_prior)

    ## Determined Variables
    μ = α .+ X * β

    ## Likelihood
    y ~ MvNormal(μ, σ)
end
```

Now that we have our model defined, we can use MCMC to perform inference. The following code evaluates the model for our data and priors. The priors in our model parameters will be wide as to allow the model to have a flexible fit. Then we take 1000 samples from the posterior, compute the posterior means of our parameters, and compute the predicted day of year values.

```Julia
# Define Model Function
mod_spline = model_spline(X, y, 100, 10, 5);

# Extract model coeffeficient indexes
_, mod_spline_ix = bijector(mod_spline, Val(true));

# MCMC
chain_spline = sample(mod_spline, NUTS(), 1_000);

# Compute means for coefficients
chain_spline_mean = mean(chain_spline).nt[:mean];

# Extract Desired Coefficients
α_spline = chain_spline_mean[mod_spline_ix[:α]...];
β_spline = chain_spline_mean[mod_spline_ix[:β]...];

# Compute Predictions
y_pred_spline = α_spline .+ X * β_spline;
```

The plots below displays the resulting weighed basis functions, and the resulting spline fit.

```Julia
# Plot Weighted Basis 
plot(x, β_spline .* Basis, title = "Weighted Basis Functions", legend = false)
ylabel!("Basis * Weight")
xlabel!("Year")
```

```Julia
# Plot Data and Spline
PlotData(alpha = 0.20)
plot!(x, y_pred_spline, color = :black, linewidth = 3, label = "Spline")
```

From the plot above, it can be seen the the resulting spline fit to the data is rather "wiggly" and not smooth. We can employ a mixed effect, or hierarchical, model to produce a smoother fit.

## Making it Smooth

Smoothness is induced by enforcing a penalty on the second derivative at the knots. The first thing we need to do is build an identity matrix with diagonal the length of the number of basis functions and twice taking the difference of that matrix.

```Julia
# Difference Matrix
# Note we take the difference twice.
D = diagm(0 => ones(n_basis)) |> 
    (x -> diff(x,dims=1)) |>
    (x -> diff(x,dims=1))
```

In order to leverage the mixed effect model, we need to reparameterize our model.
Wood goes into detail on the theory of this parameterization on pages 173-174 of his text.
The following Julia code executes the reparameterized design and penalty matrix. The resulting `X_fe` represents the fixed effect matrix and `Z_re` the random effects (hierarchcial) matrix.

```Julia
# make D_+
D_pls = vcat(zeros(2,n_basis), D);
D_pls[diagind(D_pls)] .= 1;

# reparameterize
XSD = (UpperTriangular(D_pls') \ X')';

# the random and fixed effects
X_fe = XSD[:, 1:2]; # fixed effects
Z_re = XSD[:, 3:end]; # random effects
```

We can now define another Turing model to fit the mixed effect smoothing spline model. This model function accepts the following arguments:

  - `X_fe` is the fixed effect matrix;
  - `Z_re` is the random effect matrix;
  - `y` is the element we want to predict;
  - `μ_int` is the center of the intercept prior;
  - `σ_prior` is the standard deviation of coeffecient priors;
  - `err_prior` defines the exponential prior on likelihood error;
  - `re_prior` defines the exponential prior on random effect variance.


```Julia
# Define model
@model function model_smooth_spline(X_fe, Z_re, y, μ_int, σ_prior, err_prior, re_prior)

    ## Priors 
    # Intercept
    α ~ Normal(μ_int, σ_prior)

    # Fixed Effects
    B ~ MvNormal(repeat([0],2), σ_prior)
    
    # Random Effects
    μ_b ~ Normal(0, σ_prior)
    σ_b ~ Exponential(re_prior)
    b ~ filldist(Normal(μ_b, σ_b), size(Z_re)[2])
    
    # error
    σ ~ Exponential(err_prior)

    ## Determined Variables
    μ = α .+ X_fe * B + Z_re * b
    
    ## Liklihood
    y ~ MvNormal(μ, σ)
end;
```

And use MCMC to perform inference as before.

```Julia
# Define Model Function
mod_smooth = model_smooth_spline(X_fe, Z_re, y, 100, 10, 5, 5);

# Extract model coeffeficient indexes
_, mod_smooth_ix = bijector(mod_smooth, Val(true));

# MCMC
chain_smooth = sample(mod_smooth, NUTS(), 1_000);

# Compute means for coefficients
chain_smooth_mean = mean(chain_smooth).nt[:mean];

# Extract Desired Coefficients
α_smooth = chain_smooth_mean[mod_smooth_ix[:α]...];
B_smooth = chain_smooth_mean[vcat(map(x -> union(mod_smooth_ix[x]...), [:B,:b])...)];

# Compute Predictions
y_pred_smooth = α_smooth .+ hcat(X_fe,Z_re)*B_smooth;
```

The following plot compares the original spline model to the mixed effect smoothing spline model that we just fit. As we can see, the smoothing spline fit looks far smoother despite having the exact same number of knots and basis funcions as the original model.

```Julia
# Plot Data and Splines
PlotData(alpha = 0.20)
plot!(x, y_pred_spline, color = :black, linewidth = 3, label = "Spline")
plot!(x, y_pred_smooth, color = :red, linewidth = 3, label = "Smoothing Spline")
```

```julia, echo=false, skip="notebook"
if isdefined(Main, :TuringTutorials)
    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])
end
```